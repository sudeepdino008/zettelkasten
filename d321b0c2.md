---
date: 2020-12-30T13:25
tags: 
  - books
---

# The User Illusion

## Computation

### Maxwell's Demon
- Grand unification: The next one in which gravity might be integrated with the other forces is not going to tell much about our everyday lives (that we don't already know). 
- In 1980s, this tendency of science world towards unification was diverted by discovery of a range of new theories - chaos, fractals, complexity, organization; These once again turn the spotlight to our daily lives.
- Maxwell's demon and 2nd law of thermodynamics
- Entropy: measure of how unavailable an amount of energy is [This definition is specific to heat, which is also used to predominately describe the laws of thermodynamics.]
- Maxwell-Boltzmann distribution: a chi distribution. At higher temperatures, there's a wider number of possible states for the individual atoms in the gas chamber.

### Throwing away information
 - Charles Bennett showed that obtaining knowledge doesn't cost much. This disproved Szilard's idea that the Maxwell Demon's still follows 2nd law because it takes energy to gain information (about the position of the atoms).
- In Maxwell's Demon, what costs is not gaining information, but losing it.
- This contradicts our everyday day view that information is a position thing. But at times, it's precisely losing information that makes the reduced form more valuable. For example: the total bill is more valuable [and you don't care about the individual costs, which is lost in the total amount].
- Boltzmann's idea of macrostates and microstates. These depend on how you describe them, but it's not subjective--Different physicists can describe macrostate as temperature, and microstate as x number of atoms, and still come up with same values.  
Entropy is number of microstates that can be represented by some macrostate.
- Link between entropy and probability - there are much more states with higher entropy. So it makes sense that the world is moving towards a state of higher entropy.
- 2nd law of thermodynamics essentially says that the world is getting harder to describe.
- Shannon initially described information as the amount of information it could represent (very similar to entropy). But unfortunately the sign of information got reversed, and is often meant as the amount of order (rather 

pg.43
